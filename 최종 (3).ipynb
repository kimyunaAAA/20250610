{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWfS4YrqLjEj",
        "outputId": "138e66b5-3381-4dfb-95d7-303a545ec636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-comment-downloader\n",
            "  Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dateparser (from youtube-comment-downloader)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-comment-downloader) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser->youtube-comment-downloader) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->youtube-comment-downloader) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser->youtube-comment-downloader) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->youtube-comment-downloader) (5.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-comment-downloader) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-comment-downloader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-comment-downloader) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-comment-downloader) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser->youtube-comment-downloader) (1.17.0)\n",
            "Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl (8.2 kB)\n",
            "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser, youtube-comment-downloader\n",
            "Successfully installed dateparser-1.2.1 youtube-comment-downloader-0.1.76\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (2.0.2)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=b0c607733b0e08bc1fba6bf373a9069a868becf1925c5223c84cc3b814324681\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n",
            "Requirement already satisfied: langcodes in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes) (75.2.0)\n"
          ]
        }
      ],
      "source": [
        " # YouTube ëŒ“ê¸€ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install youtube-comment-downloader\n",
        "\n",
        "# ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (langid: í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)\n",
        "!pip install langid\n",
        "\n",
        "# Streamlit ì„¤ì¹˜ (Streamlitì€ ê°„ë‹¨í•œ ì›¹ ì•±ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)\n",
        "!pip install streamlit\n",
        "\n",
        "# ngrokì„ í†µí•´ ë¡œì»¬ ì„œë²„ë¥¼ ì™¸ë¶€ì—ì„œë„ ì ‘ì† ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (streamlit ì›¹ì•±ì„ ë°°í¬í•  ë•Œ ì‚¬ìš© ê°€ëŠ¥)\n",
        "!pip install pyngrok\n",
        "\n",
        "# ì–¸ì–´ ì½”ë“œ ê´€ë ¨ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (langcodes: ISO 639 ì–¸ì–´ ì½”ë“œ ê´€ë¦¬ ë° ë³€í™˜)\n",
        "!pip install langcodes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê¸°ì¡´ ë²„ì „, ì§€ê¸ˆ ì•ˆ ì”€"
      ],
      "metadata": {
        "id": "kbFdYxLC5paz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdB8eQ1yLzX6",
        "outputId": "7c542aa2-2af7-44b2-fbf2-81b66a114c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "#ì§€ê¸ˆ ì‘ì„±í•œ ì½”ë“œ ë¸”ë¡ì„ íŒŒì¼ë¡œ ì €ì¥í•´ ì£¼ëŠ” ê¸°ëŠ¥\n",
        "%%writefile streamlit_app.py\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "import streamlit as st  # Streamlit: ì›¹ì•±ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import os  # OS ëª…ë ¹ì–´ ì‹¤í–‰ìš©\n",
        "import json  # JSON ë°ì´í„° ì²˜ë¦¬\n",
        "import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬\n",
        "import langid  # ì–¸ì–´ ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import langcodes  # ì–¸ì–´ ì½”ë“œ -> ì–¸ì–´ ì´ë¦„ ë§¤í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "\n",
        "# ë””ìì¸ CSS ì¶”ê°€: ì›¹ì•±ì˜ ë°°ê²½, í°íŠ¸, ë²„íŠ¼ ë“± ì‹œê°ì  ìŠ¤íƒ€ì¼ ì •ì˜\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "/* ë°°ê²½ ë° ê¸€ê¼´ ì„¤ì • */\n",
        "body {\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "  background-color: #ffffff;\n",
        "  text-align: center;\n",
        "  color: #333;\n",
        "}\n",
        "/* ì¤‘ì•™ ë¡œê³  ë° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */\n",
        ".center-box {\n",
        "  display: flex;\n",
        "  flex-direction: column;\n",
        "  justify-content: center;\n",
        "  align-items: center;\n",
        "  margin-top: 50px;\n",
        "}\n",
        ".center-box img {\n",
        "  width: 40px;\n",
        "  margin-bottom: 20px;\n",
        "}\n",
        ".center-box .title {\n",
        "  font-size: 48px;\n",
        "  color: #e60000;\n",
        "  font-weight: bold;\n",
        "  margin: 10px 0;\n",
        "}\n",
        ".center-box .subtitle {\n",
        "  font-size: 16px;\n",
        "  color: #888;\n",
        "  margin-bottom: 30px;\n",
        "}\n",
        "/* ì…ë ¥ë€ê³¼ ë²„íŠ¼ ìŠ¤íƒ€ì¼ */\n",
        ".input-style input {\n",
        "  padding: 12px 20px;\n",
        "  width: 300px;\n",
        "  border-radius: 10px;\n",
        "  border: 1px solid #ccc;\n",
        "  font-size: 14px;\n",
        "}\n",
        ".button-style button {\n",
        "  padding: 12px 20px;\n",
        "  background-color: #e60000;\n",
        "  color: white;\n",
        "  border: none;\n",
        "  border-radius: 10px;\n",
        "  font-weight: bold;\n",
        "  cursor: pointer;\n",
        "  font-size: 14px;\n",
        "}\n",
        ".button-style button:hover {\n",
        "  background-color: #cc0000;\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ì¤‘ì•™ ë¡œê³ , ì œëª©, ì„¤ëª… í‘œì‹œ\n",
        "st.markdown(\"\"\"\n",
        "<div class=\"center-box\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b8/YouTube_Logo_2017.svg\" alt=\"YouTube logo\" />\n",
        "  <div class=\"title\">YouniversAI</div>\n",
        "  <div class=\"subtitle\">Your smart assistant for multilingual YouTube comment insights.</div>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "\n",
        "# YouTube ëŒ“ê¸€ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "def get_comments(url):\n",
        "    json_file = 'YoutubeComments.json'\n",
        "    os.system(f'youtube-comment-downloader --url \"{url}\" --output {json_file}')  # ì™¸ë¶€ ëª…ë ¹ì–´ë¡œ ëŒ“ê¸€ ë‹¤ìš´ë¡œë“œ (youtube-comment-downloader ì‚¬ìš©)\n",
        "\n",
        "    # JSON íŒŒì¼ ì½ê¸°\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "        try:\n",
        "            json_data = json.loads(content)  # í‘œì¤€ JSON íŒŒì‹±\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON ë¼ì¸ ë‹¨ìœ„ íŒŒì‹± (ë¹„í‘œì¤€ í˜•ì‹ ì²˜ë¦¬)\n",
        "            json_data = []\n",
        "            for line in content.splitlines():\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    json_data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "    df = pd.DataFrame(json_data)  # pandas DataFrame ìƒì„±\n",
        "    os.remove(json_file)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
        "    return df\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "\n",
        "# ì–¸ì–´ ê°ì§€ ë° ì–¸ì–´ ì´ë¦„ ë³€í™˜ í•¨ìˆ˜\n",
        "def classify_language(df):\n",
        "    df['lang'] = df['text'].apply(lambda x: langid.classify(x)[0])  # langidë¡œ ì–¸ì–´ì½”ë“œ ê°ì§€\n",
        "    df['ì–¸ì–´'] = df['lang'].apply(lambda code: langcodes.Language.get(code).display_name() if code else code)  # ì–¸ì–´ ì½”ë“œ -> ì–¸ì–´ëª… ë³€í™˜\n",
        "    return df\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "# Streamlit ì›¹ì•±ì˜ ë©”ì¸ í•¨ìˆ˜\n",
        "def main():\n",
        "    # URL ì…ë ¥ë€ê³¼ ë²„íŠ¼\n",
        "    url = st.text_input(\"Paste YouTube video URL here\")\n",
        "    if st.button(\"Go\"):\n",
        "        st.write(f\"ğŸ“º You entered: {url}\")\n",
        "\n",
        "        with st.spinner(\"ğŸ”ëŒ“ê¸€ì„ ë¶„ë¥˜ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”\"):\n",
        "            df = get_comments(url)  # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°\n",
        "            if df.empty or 'text' not in df.columns:\n",
        "                st.warning(\"No comments found or 'text' field missing.\")  # ì˜¤ë¥˜ ì²˜ë¦¬\n",
        "                return\n",
        "            df = classify_language(df)  # ì–¸ì–´ ë¶„ë¥˜\n",
        "            st.session_state.df = df  # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ë‹¤ë¥¸ ì¸í„°ë™ì…˜ì—ì„œë„ ìœ ì§€)\n",
        "\n",
        "    # ëŒ“ê¸€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ì¶œë ¥\n",
        "    if 'df' in st.session_state:\n",
        "        df = st.session_state.df\n",
        "        st.success(\"ëŒ“ê¸€ ìˆ˜ì§‘ ë° ì–¸ì–´ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
        "        st.write(\"ì´ ëŒ“ê¸€ ìˆ˜:\", len(df))\n",
        "\n",
        "        # ì–¸ì–´ ì„ íƒ ë©€í‹°ì…€ë ‰íŠ¸\n",
        "        languages = df['lang'].unique().tolist()  # ì–¸ì–´ ì½”ë“œ ë¦¬ìŠ¤íŠ¸\n",
        "        language_options = [langcodes.Language.get(code).display_name() for code in languages]\n",
        "\n",
        "        selected_langs = st.multiselect(\"ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”\", options=language_options, default=language_options)\n",
        "\n",
        "        # ì„ íƒëœ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ í•„í„°ë§\n",
        "        selected_lang_codes = [code for code in languages if langcodes.Language.get(code).display_name() in selected_langs]\n",
        "        filtered_df = df[df['lang'].isin(selected_lang_codes)]\n",
        "\n",
        "        # ì„ íƒëœ ì–¸ì–´ì˜ ëŒ“ê¸€ ì¶œë ¥\n",
        "        if not filtered_df.empty:\n",
        "            st.write(filtered_df[['ì–¸ì–´', 'text']])\n",
        "        else:\n",
        "            st.warning(\"ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#____________________________________________________________________________\n",
        "\n",
        "# í”„ë¡œê·¸ë¨ ì‹œì‘ì \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbOMkRJwGDrS"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DyamhTIAsp"
      },
      "source": [
        "**ë””ìì¸**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjz5XQ2RGDDs",
        "outputId": "7a74b5dc-e021-4ac2-ed54-9f953606f05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing design.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile design.py\n",
        "import streamlit as st\n",
        "\n",
        "def apply_css():\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    body {\n",
        "      margin: 0;\n",
        "      padding: 0;\n",
        "      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "      background-color: #ffffff;\n",
        "      text-align: center;\n",
        "      color: #333;\n",
        "    }\n",
        "    .center-box {\n",
        "      display: flex;\n",
        "      flex-direction: column;\n",
        "      justify-content: center;\n",
        "      align-items: center;\n",
        "      margin-top: 50px;\n",
        "    }\n",
        "    .center-box img {\n",
        "      width: 40px;\n",
        "      margin-bottom: 20px;\n",
        "    }\n",
        "    .center-box .title {\n",
        "      font-size: 48px;\n",
        "      color: #e60000;\n",
        "      font-weight: bold;\n",
        "      margin: 10px 0;\n",
        "    }\n",
        "    .center-box .subtitle {\n",
        "      font-size: 16px;\n",
        "      color: #888;\n",
        "      margin-bottom: 30px;\n",
        "    }\n",
        "    .input-style input {\n",
        "      padding: 12px 20px;\n",
        "      width: 300px;\n",
        "      border-radius: 10px;\n",
        "      border: 1px solid #ccc;\n",
        "      font-size: 14px;\n",
        "    }\n",
        "    .button-style button {\n",
        "      padding: 12px 20px;\n",
        "      background-color: #e60000;\n",
        "      color: white;\n",
        "      border: none;\n",
        "      border-radius: 10px;\n",
        "      font-weight: bold;\n",
        "      cursor: pointer;\n",
        "      font-size: 14px;\n",
        "    }\n",
        "    .button-style button:hover {\n",
        "      background-color: #cc0000;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "def show_header():\n",
        "    st.markdown(\"\"\"\n",
        "    <div class=\"center-box\">\n",
        "      <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b8/YouTube_Logo_2017.svg\" alt=\"YouTube logo\" />\n",
        "      <div class=\"title\">YouniversAI</div>\n",
        "      <div class=\"subtitle\">Your smart assistant for multilingual YouTube comment insights.</div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5sWTgNrH_Ei"
      },
      "source": [
        "**ëŒ“ê¸€ ìˆ˜ì§‘**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9_5DI97H9eI",
        "outputId": "52355102-0b90-47bb-fc3d-f7f63afb7e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing logic.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile logic.py\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def get_comments(url):\n",
        "    json_file = 'YoutubeComments.json'\n",
        "    os.system(f'youtube-comment-downloader --url \"{url}\" --output {json_file}')\n",
        "\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "        try:\n",
        "            json_data = json.loads(content)\n",
        "        except json.JSONDecodeError:\n",
        "            json_data = []\n",
        "            for line in content.splitlines():\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    json_data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "    df = pd.DataFrame(json_data)\n",
        "    os.remove(json_file)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-zFjsmPIEmR"
      },
      "source": [
        "**ëŒ“ê¸€ ë¶„ë¥˜**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt1reUQ3IEBl",
        "outputId": "ae35c5d3-04fc-4cd0-d317-4099ecae6f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing classifier.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile classifier.py\n",
        "import langid\n",
        "import langcodes\n",
        "\n",
        "def classify_language(df):\n",
        "    df['lang'] = df['text'].apply(lambda x: langid.classify(x)[0])\n",
        "    df['ì–¸ì–´'] = df['lang'].apply(\n",
        "        lambda code: langcodes.Language.get(code).display_name() if code else code)\n",
        "    return df\n",
        "\n",
        "def classify_and_store(df, session_state):\n",
        "    df = classify_language(df)\n",
        "    session_state.df = df\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh3N41DMJvWk"
      },
      "source": [
        "**ìš”ì•½ ì „ìš© íŒŒì¼**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0sodLIiJ0Vt",
        "outputId": "7dd7f535-a3a7-48a9-9e70-943124b230df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing summarizer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile summarizer.py\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        "    device=-1  # CPU ëª¨ë“œ\n",
        ")\n",
        "\n",
        "# í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
        "kw_model = KeyBERT(model=embed_model)\n",
        "\n",
        "def summarize_by_language(df):\n",
        "    result = {}\n",
        "    # summarizer.py ìˆ˜ì •\n",
        "    grouped = df.groupby('lang', dropna=True)\n",
        "\n",
        "\n",
        "    for lang, group in grouped:\n",
        "        comments = group['text'].dropna().astype(str).tolist()\n",
        "        full_text = \" \".join(comments)\n",
        "\n",
        "        if len(full_text) < 50:\n",
        "            summary = \"(ëŒ“ê¸€ì´ ë„ˆë¬´ ì ì–´ ìš”ì•½ ë¶ˆê°€)\"\n",
        "            keywords = \"\"\n",
        "        else:\n",
        "            try:\n",
        "                summary = summarizer(full_text[:1500], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
        "            except:\n",
        "                summary = \"(ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ)\"\n",
        "            try:\n",
        "                keywords = \", \".join([kw[0] for kw in kw_model.extract_keywords(full_text)])\n",
        "            except:\n",
        "                keywords = \"\"\n",
        "\n",
        "        result[lang] = {\n",
        "            \"summary\": summary,\n",
        "            \"keywords\": keywords\n",
        "        }\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizUKdFpRCea"
      },
      "source": [
        "**ê°ì • ë¶„ì„ ì „ìš© íŒŒì¼**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ceG2zeREgt",
        "outputId": "a2807f5c-207d-48d6-c0f6-593eb229f948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sentiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sentiment.py\n",
        "import langid\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# ê°ì • ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™” (CPU ëª¨ë“œ)\n",
        "sentiment_model = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=sentiment_model,\n",
        "    tokenizer=sentiment_model,\n",
        "    truncation=True,\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "# ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ (ê°„ë‹¨ ë²„ì „)\n",
        "def detect_langid(text):\n",
        "    try:\n",
        "        text = str(text).strip()\n",
        "        if len(text) < 2:\n",
        "            return 'unknown'\n",
        "        return langid.classify(text)[0]\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "# ê°ì • ë¶„ì„ í•¨ìˆ˜\n",
        "def analyze_sentiment(df):\n",
        "    df['language'] = df['text'].apply(detect_langid)\n",
        "    dfs_by_lang = {}\n",
        "\n",
        "    for lang in df['language'].unique():\n",
        "        df_lang = df[df['language'] == lang].copy()\n",
        "        if df_lang.empty:\n",
        "            continue\n",
        "\n",
        "        comments = df_lang['text'].dropna().astype(str).tolist()\n",
        "        if not comments:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            results = sentiment_pipeline(comments)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        df_lang['sentiment'] = [r['label'] for r in results]\n",
        "        df_lang['score'] = [r['score'] for r in results]\n",
        "        df_lang['sentiment_score'] = df_lang['sentiment'].apply(lambda l: int(l.split()[0]))\n",
        "        df_lang['sentiment_kor'] = df_lang['sentiment_score'].apply(\n",
        "            lambda s: \"ë¶€ì •\" if s <= 2 else (\"ì¤‘ë¦½\" if s == 3 else \"ê¸ì •\")\n",
        "        )\n",
        "\n",
        "        dfs_by_lang[lang] = df_lang\n",
        "\n",
        "    if not dfs_by_lang:\n",
        "        return df.copy()\n",
        "\n",
        "    return pd.concat(dfs_by_lang.values(), ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____________________________________________________________________________"
      ],
      "metadata": {
        "id": "oqGKBmEM50HS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrzhh2YyIMik"
      },
      "source": [
        "# ë©”ì¸ ì‹¤í–‰ íŒŒì¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ArGCuUIKg7",
        "outputId": "8d43d1b0-f601-4981-c061-5c56294a7c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import langcodes\n",
        "from design import apply_css, show_header\n",
        "from logic import get_comments\n",
        "from classifier import classify_and_store\n",
        "\n",
        "def main():\n",
        "    apply_css()\n",
        "    show_header()\n",
        "\n",
        "    url = st.text_input(\"Paste YouTube video URL here\")\n",
        "    if st.button(\"Go\"):\n",
        "        st.write(f\"ğŸ“º You entered: {url}\")\n",
        "        with st.spinner(\"ğŸ”ëŒ“ê¸€ì„ ë¶„ë¥˜ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”\"):\n",
        "            df = get_comments(url)\n",
        "            if df.empty or 'text' not in df.columns:\n",
        "                st.warning(\"No comments found or 'text' field missing.\")\n",
        "                return\n",
        "            classify_and_store(df, st.session_state)\n",
        "\n",
        "    if 'df' in st.session_state:\n",
        "        df = st.session_state.df\n",
        "        st.success(\"ëŒ“ê¸€ ìˆ˜ì§‘ ë° ì–¸ì–´ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
        "        st.write(\"ì´ ëŒ“ê¸€ ìˆ˜:\", len(df))\n",
        "\n",
        "        languages = df['lang'].unique().tolist()\n",
        "        language_options = [langcodes.Language.get(code).display_name() for code in languages]\n",
        "\n",
        "        selected_langs = st.multiselect(\"ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”\", options=language_options, default=language_options)\n",
        "        selected_lang_codes = [code for code in languages if langcodes.Language.get(code).display_name() in selected_langs]\n",
        "        filtered_df = df[df['lang'].isin(selected_lang_codes)]\n",
        "\n",
        "        if not filtered_df.empty:\n",
        "            st.write(filtered_df[['ì–¸ì–´', 'text']])\n",
        "        else:\n",
        "            st.warning(\"ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4nQ38z1KFbb"
      },
      "source": [
        "ë©”ì¸ ì‹¤í–‰ íŒŒì¼ - ìš”ì•½ ì „ìš© íŒŒì¼ ì ìš© ë²„ì „"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vMN3a5oKlUq",
        "outputId": "097a3e5a-ad0b-45cf-cb08-7595765c1d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m680.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keybert sentence-transformers transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cCs_xmKJ87",
        "outputId": "b86f6698-0859-4191-edf8-329ea53f8bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import langcodes\n",
        "from design import apply_css, show_header\n",
        "from logic import get_comments\n",
        "from classifier import classify_and_store\n",
        "from summarizer import summarize_by_language  # ìš”ì•½ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "def main():\n",
        "    apply_css()\n",
        "    show_header()\n",
        "\n",
        "    url = st.text_input(\"Paste YouTube video URL here\")\n",
        "    if st.button(\"Go\"):\n",
        "        st.write(f\"ğŸ“º You entered: {url}\")\n",
        "        with st.spinner(\"ğŸ”ëŒ“ê¸€ì„ ë¶„ë¥˜ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”\"):\n",
        "            df = get_comments(url)\n",
        "            if df.empty or 'text' not in df.columns:\n",
        "                st.warning(\"No comments found or 'text' field missing.\")\n",
        "                return\n",
        "            classify_and_store(df, st.session_state)\n",
        "\n",
        "    if 'df' in st.session_state:\n",
        "        df = st.session_state.df\n",
        "        st.success(\"ëŒ“ê¸€ ìˆ˜ì§‘ ë° ì–¸ì–´ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
        "        st.write(\"ì´ ëŒ“ê¸€ ìˆ˜:\", len(df))\n",
        "\n",
        "        languages = df['lang'].unique().tolist()\n",
        "        language_options = [langcodes.Language.get(code).display_name() for code in languages]\n",
        "\n",
        "        selected_langs = st.multiselect(\"ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”\", options=language_options, default=language_options)\n",
        "        selected_lang_codes = [code for code in languages if langcodes.Language.get(code).display_name() in selected_langs]\n",
        "        filtered_df = df[df['lang'].isin(selected_lang_codes)]\n",
        "\n",
        "        if not filtered_df.empty:\n",
        "            st.write(filtered_df[['ì–¸ì–´', 'text']])\n",
        "\n",
        "            # âœ… ìš”ì•½í•˜ê¸° ë²„íŠ¼\n",
        "            if st.button(\"ìš”ì•½í•˜ê¸°\"):\n",
        "                st.session_state.summary = summarize_by_language(filtered_df)\n",
        "\n",
        "        else:\n",
        "            st.warning(\"ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        # âœ… ìš”ì•½ ê²°ê³¼ ì¶œë ¥ (ë²„íŠ¼ í´ë¦­ ì´í›„ì—ë§Œ í‘œì‹œë¨)\n",
        "        if 'summary' in st.session_state and st.session_state.summary:\n",
        "            for code in selected_lang_codes:\n",
        "                summary = st.session_state.summary.get(code)\n",
        "                lang_name = langcodes.Language.get(code).display_name()\n",
        "\n",
        "                st.markdown(f\"### ğŸ’¬ {lang_name} ìš”ì•½\")\n",
        "                if summary:\n",
        "                    st.markdown(f\"**ìš”ì•½:** {summary.get('summary', '(ìš”ì•½ ì—†ìŒ)')}\")\n",
        "                    st.markdown(f\"**í‚¤ì›Œë“œ:** {summary.get('keywords', '(í‚¤ì›Œë“œ ì—†ìŒ)')}\")\n",
        "                else:\n",
        "                    st.info(\"ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqTETxsoRnUv"
      },
      "source": [
        "ë©”ì¸ ì‹¤í–‰ íŒŒì¼ - ìš”ì•½ ì „ìš© íŒŒì¼ + ê°ì • ë¶„ì„ ì „ìš© íŒŒì¼ ì ìš© ë²„ì „"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ„ì— '!pip install keybert sentence-transformers transformers --quiet'ê°€ ì‹¤í–‰ë˜ì–´ ìˆì–´ì•¼ í•¨"
      ],
      "metadata": {
        "id": "BblAB5IY574E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJJ7Jzi_R1ka",
        "outputId": "66ab61fa-b3dc-454c-b8f5-c9a6d5646720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import langcodes\n",
        "from design import apply_css, show_header\n",
        "from logic import get_comments\n",
        "from classifier import classify_and_store\n",
        "from summarizer import summarize_by_language\n",
        "from sentiment import analyze_sentiment  # âœ… ê°ì • ë¶„ì„ ì¶”ê°€\n",
        "\n",
        "def main():\n",
        "    apply_css()\n",
        "    show_header()\n",
        "\n",
        "    url = st.text_input(\"Paste YouTube video URL here\")\n",
        "    if st.button(\"Go\"):\n",
        "        st.write(f\"\\U0001F4FA You entered: {url}\")\n",
        "        with st.spinner(\"\\U0001F50EëŒ“ê¸€ì„ ë¶„ë¥˜ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”\"):\n",
        "            df = get_comments(url)\n",
        "            if df.empty or 'text' not in df.columns:\n",
        "                st.warning(\"No comments found or 'text' field missing.\")\n",
        "                return\n",
        "            classify_and_store(df, st.session_state)\n",
        "\n",
        "    if 'df' in st.session_state:\n",
        "        df = st.session_state.df\n",
        "        st.success(\"ëŒ“ê¸€ ìˆ˜ì§‘ ë° ì–¸ì–´ ë¶„ë¥˜ ì™„ë£Œ!\")\n",
        "        st.write(\"ì´ ëŒ“ê¸€ ìˆ˜:\", len(df))\n",
        "\n",
        "        languages = df['lang'].unique().tolist()\n",
        "        language_options = [langcodes.Language.get(code).display_name() for code in languages]\n",
        "\n",
        "        selected_langs = st.multiselect(\"ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”\", options=language_options, default=language_options)\n",
        "        selected_lang_codes = [code for code in languages if langcodes.Language.get(code).display_name() in selected_langs]\n",
        "        filtered_df = df[df['lang'].isin(selected_lang_codes)]\n",
        "\n",
        "        if not filtered_df.empty:\n",
        "            st.write(filtered_df[['ì–¸ì–´', 'text']])\n",
        "\n",
        "            # âœ… ê°ì • ë¶„ì„ ë²„íŠ¼ ì¶”ê°€\n",
        "            if st.button(\"ê°ì • ë¶„ì„í•˜ê¸°\"):\n",
        "                with st.spinner(\"\\U0001F914 ê°ì •ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...\"):\n",
        "                    df_with_sentiment = analyze_sentiment(filtered_df)\n",
        "                    st.session_state.df = df_with_sentiment  # ì—…ë°ì´íŠ¸\n",
        "                    st.success(\"ê°ì • ë¶„ì„ ì™„ë£Œ!\")\n",
        "                    st.dataframe(df_with_sentiment[['ì–¸ì–´', 'text', 'sentiment_kor']])\n",
        "\n",
        "            # âœ… ìš”ì•½ ë²„íŠ¼ ì¶”ê°€\n",
        "            if st.button(\"ìš”ì•½í•˜ê¸°\"):\n",
        "                with st.spinner(\"\\u23F3 ì–¸ì–´ë³„ ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤...\"):\n",
        "                    st.session_state.summary = summarize_by_language(filtered_df)\n",
        "\n",
        "                for lang_code in selected_lang_codes:\n",
        "                    summary = st.session_state.summary.get(lang_code)\n",
        "                    lang_label = langcodes.Language.get(lang_code).display_name()\n",
        "                    st.markdown(f\"### ğŸ’¬ {lang_label} ìš”ì•½\")\n",
        "                    if summary:\n",
        "                        st.markdown(f\"**ìš”ì•½:** {summary.get('summary', '(ìš”ì•½ ì—†ìŒ)')}\")\n",
        "                        st.markdown(f\"**í‚¤ì›Œë“œ:** {summary.get('keywords', '(í‚¤ì›Œë“œ ì—†ìŒ)')}\")\n",
        "                    else:\n",
        "                        st.info(\"ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        else:\n",
        "            st.warning(\"ì„ íƒí•œ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________"
      ],
      "metadata": {
        "id": "_EnTQana6Fer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TWEDbaTRL-Jx"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok  # pyngrok ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ì—¬ ngrokê³¼ ì—°ë™\n",
        "\n",
        "\n",
        "# ngrok í† í° ë“±ë¡ (ì—¬ê¸°ì— ë³¸ì¸ ngrok ê³„ì •ì˜ ì¸ì¦ í† í°ì„ ì…ë ¥í•´ì•¼ í•¨)\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "\n",
        "\n",
        "# ì—°ê²°í•  í¬íŠ¸ë¥¼ ì§€ì •í•˜ê³ , ngrokìœ¼ë¡œ í•´ë‹¹ í¬íŠ¸ë¥¼ ì™¸ë¶€ì—ì„œ ì ‘ì† ê°€ëŠ¥í•˜ê²Œ ì—°ê²°\n",
        "port = 8501  # Streamlitì˜ ê¸°ë³¸ í¬íŠ¸ê°€ 8501\n",
        "public_url = ngrok.connect(port)  # í¬íŠ¸ 8501ì„ ì™¸ë¶€ ì ‘ì†ìš©ìœ¼ë¡œ ê³µê°œ\n",
        "print(f\"Public URL: {public_url}\")  # ìƒì„±ëœ public URLì„ ì¶œë ¥\n",
        "\n",
        "\n",
        "\n",
        "# Streamlit ì•± ì‹¤í–‰ (streamlit_app.py íŒŒì¼ì„ ì‹¤í–‰í•˜ê³ , ì§€ì •ëœ í¬íŠ¸ë¡œ ì„œë²„ë¥¼ ì‹œì‘)\n",
        "# &ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Colab ê°™ì€ í™˜ê²½ì—ì„œëŠ” í•„ìš”)\n",
        "!streamlit run streamlit_app.py --server.port {port} &\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}